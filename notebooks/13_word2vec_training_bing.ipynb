{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wd5oGr_PP4an"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time \n",
    "import multiprocessing\n",
    "import logging  # logger\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>wall st bear claw back black reuters reuters short seller wall street dwindling band ultra cynic seeing green</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>carlyle look toward commercial aerospace reuters reuters private investment firm carlyle group reputation making well timed occasionally controversial play defense industry quietly placed bet another part market</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>oil economy cloud stock outlook reuters reuters soaring crude price plus worry economy outlook earnings expected hang stock market next week depth summer doldrums</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>iraq halt oil export main southern pipeline reuters reuters authority halted oil export flow main pipeline southern iraq intelligence showed rebel militia strike infrastructure oil official said saturday</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>oil price soar time record posing new menace economy afp afp tearaway world oil price toppling record straining wallet present new economic menace barely three month presidential election</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  \\\n",
       "0      business   \n",
       "1      business   \n",
       "2      business   \n",
       "3      business   \n",
       "4      business   \n",
       "\n",
       "                                                                                                                                                                                                             Documents  \\\n",
       "0                                                                                                        wall st bear claw back black reuters reuters short seller wall street dwindling band ultra cynic seeing green   \n",
       "1  carlyle look toward commercial aerospace reuters reuters private investment firm carlyle group reputation making well timed occasionally controversial play defense industry quietly placed bet another part market   \n",
       "2                                                   oil economy cloud stock outlook reuters reuters soaring crude price plus worry economy outlook earnings expected hang stock market next week depth summer doldrums   \n",
       "3          iraq halt oil export main southern pipeline reuters reuters authority halted oil export flow main pipeline southern iraq intelligence showed rebel militia strike infrastructure oil official said saturday   \n",
       "4                          oil price soar time record posing new menace economy afp afp tearaway world oil price toppling record straining wallet present new economic menace barely three month presidential election   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          18             4          12                3           2  \n",
       "1          27             5          15                4           3  \n",
       "2          24             5          17                4           2  \n",
       "3          28             3          19                6           3  \n",
       "4          28             4          16                7           3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cleaned_AG.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Title & Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>[wall, st, bear, claw, back, black, reuters, reuters, short, seller, wall, street, dwindling, band, ultra, cynic, seeing, green]</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>[carlyle, look, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, play, defense, industry, quietly, placed, bet, another, part, market]</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>[oil, economy, cloud, stock, outlook, reuters, reuters, soaring, crude, price, plus, worry, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>[iraq, halt, oil, export, main, southern, pipeline, reuters, reuters, authority, halted, oil, export, flow, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, strike, infrastructure, oil, official, said, saturday]</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>[oil, price, soar, time, record, posing, new, menace, economy, afp, afp, tearaway, world, oil, price, toppling, record, straining, wallet, present, new, economic, menace, barely, three, month, presidential, election]</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  \\\n",
       "0      business   \n",
       "1      business   \n",
       "2      business   \n",
       "3      business   \n",
       "4      business   \n",
       "\n",
       "                                                                                                                                                                                                                                         Documents  \\\n",
       "0                                                                                                                 [wall, st, bear, claw, back, black, reuters, reuters, short, seller, wall, street, dwindling, band, ultra, cynic, seeing, green]   \n",
       "1  [carlyle, look, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, play, defense, industry, quietly, placed, bet, another, part, market]   \n",
       "2                                                      [oil, economy, cloud, stock, outlook, reuters, reuters, soaring, crude, price, plus, worry, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]   \n",
       "3         [iraq, halt, oil, export, main, southern, pipeline, reuters, reuters, authority, halted, oil, export, flow, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, strike, infrastructure, oil, official, said, saturday]   \n",
       "4                         [oil, price, soar, time, record, posing, new, menace, economy, afp, afp, tearaway, world, oil, price, toppling, record, straining, wallet, present, new, economic, menace, barely, three, month, presidential, election]   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          18             4          12                3           2  \n",
       "1          27             5          15                4           3  \n",
       "2          24             5          17                4           2  \n",
       "3          28             3          19                6           3  \n",
       "4          28             4          16                7           3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents'] = data['Documents'].str.split(\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings using Word2Vec algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:20:26: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-04-29T22:20:26.365065', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# initializing word2vec model\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2, # window size for context \n",
    "                     vector_size=100,  # no of features \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:20:26: collecting all words and their counts\n",
      "INFO - 22:20:26: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #10000, processed 273130 words, keeping 18981 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #20000, processed 542961 words, keeping 26350 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #30000, processed 810845 words, keeping 31364 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #40000, processed 1079542 words, keeping 35363 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #50000, processed 1347719 words, keeping 38795 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #60000, processed 1617217 words, keeping 42012 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #70000, processed 1887970 words, keeping 44832 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #80000, processed 2156362 words, keeping 47460 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #90000, processed 2420763 words, keeping 50103 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #100000, processed 2685958 words, keeping 52494 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #110000, processed 2951158 words, keeping 54723 word types\n",
      "INFO - 22:20:26: PROGRESS: at sentence #120000, processed 3217943 words, keeping 56786 word types\n",
      "INFO - 22:20:27: collected 58107 word types from a corpus of 3420491 raw words and 127600 sentences\n",
      "INFO - 22:20:27: Creating a fresh vocabulary\n",
      "INFO - 22:20:27: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 12033 unique words (20.70834839176003%% of original 58107, drops 46074)', 'datetime': '2021-04-29T22:20:27.050429', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:20:27: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 3242708 word corpus (94.80241286996517%% of original 3420491, drops 177783)', 'datetime': '2021-04-29T22:20:27.051052', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:20:27: deleting the raw counts dictionary of 58107 items\n",
      "INFO - 22:20:27: sample=6e-05 downsamples 1226 most-common words\n",
      "INFO - 22:20:27: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1974354.8758255735 word corpus (60.9%% of prior 3242708)', 'datetime': '2021-04-29T22:20:27.109208', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 22:20:27: estimated required memory for 12033 words and 100 dimensions: 15642900 bytes\n",
      "INFO - 22:20:27: resetting layer weights\n",
      "INFO - 22:20:27: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T22:20:27.205744', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(data['Documents'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:20:27: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 12033 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-04-29T22:20:27.209792', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 22:20:28: EPOCH 1 - PROGRESS: at 49.07% examples, 972523 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:29: EPOCH 1 - PROGRESS: at 99.24% examples, 976533 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:29: EPOCH - 1 : training on 3420491 raw words (1974121 effective words) took 2.0s, 981439 effective words/s\n",
      "INFO - 22:20:30: EPOCH 2 - PROGRESS: at 49.07% examples, 972700 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:31: EPOCH - 2 : training on 3420491 raw words (1974532 effective words) took 2.0s, 997960 effective words/s\n",
      "INFO - 22:20:32: EPOCH 3 - PROGRESS: at 50.80% examples, 1004444 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:33: EPOCH - 3 : training on 3420491 raw words (1975240 effective words) took 1.9s, 1025987 effective words/s\n",
      "INFO - 22:20:34: EPOCH 4 - PROGRESS: at 50.22% examples, 994435 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:35: EPOCH - 4 : training on 3420491 raw words (1974375 effective words) took 1.9s, 1026909 effective words/s\n",
      "INFO - 22:20:36: EPOCH 5 - PROGRESS: at 52.21% examples, 1037554 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:36: EPOCH - 5 : training on 3420491 raw words (1975308 effective words) took 1.9s, 1048060 effective words/s\n",
      "INFO - 22:20:37: EPOCH 6 - PROGRESS: at 51.08% examples, 1014575 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:38: EPOCH - 6 : training on 3420491 raw words (1974005 effective words) took 1.9s, 1041417 effective words/s\n",
      "INFO - 22:20:39: EPOCH 7 - PROGRESS: at 49.93% examples, 987420 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:40: EPOCH - 7 : training on 3420491 raw words (1974007 effective words) took 1.9s, 1023203 effective words/s\n",
      "INFO - 22:20:41: EPOCH 8 - PROGRESS: at 52.21% examples, 1033593 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:42: EPOCH - 8 : training on 3420491 raw words (1974192 effective words) took 1.9s, 1039521 effective words/s\n",
      "INFO - 22:20:43: EPOCH 9 - PROGRESS: at 52.21% examples, 1036454 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:44: EPOCH - 9 : training on 3420491 raw words (1973680 effective words) took 1.8s, 1071142 effective words/s\n",
      "INFO - 22:20:45: EPOCH 10 - PROGRESS: at 54.26% examples, 1077792 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:46: EPOCH - 10 : training on 3420491 raw words (1975366 effective words) took 1.8s, 1101655 effective words/s\n",
      "INFO - 22:20:47: EPOCH 11 - PROGRESS: at 54.25% examples, 1078173 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:20:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:48: EPOCH - 11 : training on 3420491 raw words (1975058 effective words) took 1.8s, 1072665 effective words/s\n",
      "INFO - 22:20:49: EPOCH 12 - PROGRESS: at 54.25% examples, 1077895 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:50: EPOCH - 12 : training on 3420491 raw words (1973878 effective words) took 1.8s, 1074247 effective words/s\n",
      "INFO - 22:20:51: EPOCH 13 - PROGRESS: at 51.93% examples, 1033480 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:51: EPOCH - 13 : training on 3420491 raw words (1974499 effective words) took 1.9s, 1061101 effective words/s\n",
      "INFO - 22:20:52: EPOCH 14 - PROGRESS: at 55.13% examples, 1090128 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:53: EPOCH - 14 : training on 3420491 raw words (1974647 effective words) took 1.8s, 1102788 effective words/s\n",
      "INFO - 22:20:54: EPOCH 15 - PROGRESS: at 54.84% examples, 1083955 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:55: EPOCH - 15 : training on 3420491 raw words (1974864 effective words) took 1.8s, 1113727 effective words/s\n",
      "INFO - 22:20:56: EPOCH 16 - PROGRESS: at 55.42% examples, 1098944 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:57: EPOCH - 16 : training on 3420491 raw words (1974695 effective words) took 1.8s, 1117882 effective words/s\n",
      "INFO - 22:20:58: EPOCH 17 - PROGRESS: at 56.00% examples, 1105955 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:20:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:20:59: EPOCH - 17 : training on 3420491 raw words (1975222 effective words) took 1.8s, 1109631 effective words/s\n",
      "INFO - 22:21:00: EPOCH 18 - PROGRESS: at 54.25% examples, 1078056 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:00: EPOCH - 18 : training on 3420491 raw words (1974468 effective words) took 1.8s, 1108018 effective words/s\n",
      "INFO - 22:21:01: EPOCH 19 - PROGRESS: at 51.37% examples, 1020442 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:02: EPOCH - 19 : training on 3420491 raw words (1973187 effective words) took 1.9s, 1050994 effective words/s\n",
      "INFO - 22:21:03: EPOCH 20 - PROGRESS: at 52.80% examples, 1047542 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:04: EPOCH - 20 : training on 3420491 raw words (1974094 effective words) took 1.9s, 1037212 effective words/s\n",
      "INFO - 22:21:05: EPOCH 21 - PROGRESS: at 55.42% examples, 1095977 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:06: EPOCH - 21 : training on 3420491 raw words (1973804 effective words) took 1.8s, 1076637 effective words/s\n",
      "INFO - 22:21:07: EPOCH 22 - PROGRESS: at 53.39% examples, 1050208 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:08: EPOCH - 22 : training on 3420491 raw words (1974343 effective words) took 1.8s, 1087540 effective words/s\n",
      "INFO - 22:21:09: EPOCH 23 - PROGRESS: at 55.13% examples, 1091738 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:10: EPOCH - 23 : training on 3420491 raw words (1974323 effective words) took 1.8s, 1089814 effective words/s\n",
      "INFO - 22:21:11: EPOCH 24 - PROGRESS: at 55.42% examples, 1099183 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:11: EPOCH - 24 : training on 3420491 raw words (1975040 effective words) took 1.8s, 1093900 effective words/s\n",
      "INFO - 22:21:12: EPOCH 25 - PROGRESS: at 51.37% examples, 1020584 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:13: EPOCH - 25 : training on 3420491 raw words (1974784 effective words) took 1.9s, 1055968 effective words/s\n",
      "INFO - 22:21:14: EPOCH 26 - PROGRESS: at 52.80% examples, 1047556 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:15: EPOCH - 26 : training on 3420491 raw words (1975549 effective words) took 1.8s, 1084735 effective words/s\n",
      "INFO - 22:21:16: EPOCH 27 - PROGRESS: at 53.68% examples, 1059693 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:17: EPOCH - 27 : training on 3420491 raw words (1975480 effective words) took 1.8s, 1088836 effective words/s\n",
      "INFO - 22:21:18: EPOCH 28 - PROGRESS: at 52.80% examples, 1047846 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:19: EPOCH - 28 : training on 3420491 raw words (1974536 effective words) took 1.8s, 1093876 effective words/s\n",
      "INFO - 22:21:20: EPOCH 29 - PROGRESS: at 54.84% examples, 1085021 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:21: EPOCH - 29 : training on 3420491 raw words (1974388 effective words) took 1.8s, 1087552 effective words/s\n",
      "INFO - 22:21:22: EPOCH 30 - PROGRESS: at 53.10% examples, 1051728 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:21:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:21:22: EPOCH - 30 : training on 3420491 raw words (1973878 effective words) took 1.8s, 1067586 effective words/s\n",
      "INFO - 22:21:22: Word2Vec lifecycle event {'msg': 'training on 102614730 raw words (59235563 effective words) took 55.7s, 1063272 effective words/s', 'datetime': '2021-04-29T22:21:22.921183', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.93 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#train word2vec model \n",
    "model.train(data['Documents'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:21:22: Word2Vec lifecycle event {'fname_or_handle': 'model.bin', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-04-29T22:21:22.924489', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'saving'}\n",
      "INFO - 22:21:22: not storing attribute cum_table\n",
      "INFO - 22:21:22: saved model.bin\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base line number of features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all headlines \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    feature_vecs = np.zeros((len(words),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for word in words:\n",
    "        feature_vecs[counter] = make_feature_vec(word, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = get_avg_feature_vecs(data['Documents'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(word2vec))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    word2vec = np.delete(word2vec, nan_indices, axis=0)\n",
    "    word2vec.drop(data.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert word2vec.shape[0] == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>0.207765</td>\n",
       "      <td>0.341556</td>\n",
       "      <td>0.509732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548532</td>\n",
       "      <td>-0.113774</td>\n",
       "      <td>0.057782</td>\n",
       "      <td>-0.063844</td>\n",
       "      <td>0.389871</td>\n",
       "      <td>-0.095498</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>-0.519580</td>\n",
       "      <td>-0.109446</td>\n",
       "      <td>-0.590428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354360</td>\n",
       "      <td>-0.169752</td>\n",
       "      <td>0.193214</td>\n",
       "      <td>0.258507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112172</td>\n",
       "      <td>-0.229674</td>\n",
       "      <td>-0.244061</td>\n",
       "      <td>-0.241459</td>\n",
       "      <td>0.255334</td>\n",
       "      <td>-0.564091</td>\n",
       "      <td>-0.093670</td>\n",
       "      <td>-0.030404</td>\n",
       "      <td>-0.096065</td>\n",
       "      <td>-0.333639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557191</td>\n",
       "      <td>-0.672038</td>\n",
       "      <td>-0.211227</td>\n",
       "      <td>0.216557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458479</td>\n",
       "      <td>-0.680095</td>\n",
       "      <td>-0.661161</td>\n",
       "      <td>-0.101276</td>\n",
       "      <td>0.374314</td>\n",
       "      <td>0.456764</td>\n",
       "      <td>0.762010</td>\n",
       "      <td>-0.061789</td>\n",
       "      <td>0.723553</td>\n",
       "      <td>-0.662003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.862505</td>\n",
       "      <td>-0.846272</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.208935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306875</td>\n",
       "      <td>-1.146071</td>\n",
       "      <td>-0.945447</td>\n",
       "      <td>-0.358710</td>\n",
       "      <td>0.520460</td>\n",
       "      <td>0.908946</td>\n",
       "      <td>0.307853</td>\n",
       "      <td>-0.044196</td>\n",
       "      <td>0.347091</td>\n",
       "      <td>-0.568198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.197830</td>\n",
       "      <td>-0.712709</td>\n",
       "      <td>-0.188731</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394509</td>\n",
       "      <td>-0.347257</td>\n",
       "      <td>-0.364584</td>\n",
       "      <td>-0.188155</td>\n",
       "      <td>0.747880</td>\n",
       "      <td>0.359722</td>\n",
       "      <td>0.396012</td>\n",
       "      <td>0.213882</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>-0.355160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127595</th>\n",
       "      <td>world</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.261548</td>\n",
       "      <td>-0.682700</td>\n",
       "      <td>-0.191530</td>\n",
       "      <td>-0.016413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206953</td>\n",
       "      <td>-0.604427</td>\n",
       "      <td>0.371688</td>\n",
       "      <td>-1.071627</td>\n",
       "      <td>-0.040156</td>\n",
       "      <td>-0.400833</td>\n",
       "      <td>-0.510795</td>\n",
       "      <td>-0.125467</td>\n",
       "      <td>-0.329108</td>\n",
       "      <td>0.591055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127596</th>\n",
       "      <td>sports</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.191443</td>\n",
       "      <td>0.313274</td>\n",
       "      <td>-0.067272</td>\n",
       "      <td>0.118658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428284</td>\n",
       "      <td>-0.462002</td>\n",
       "      <td>-0.543505</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.179041</td>\n",
       "      <td>-0.142714</td>\n",
       "      <td>-0.342634</td>\n",
       "      <td>-0.062155</td>\n",
       "      <td>0.161093</td>\n",
       "      <td>-0.413196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127597</th>\n",
       "      <td>sports</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.109909</td>\n",
       "      <td>0.097675</td>\n",
       "      <td>0.284736</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510660</td>\n",
       "      <td>-0.542429</td>\n",
       "      <td>-0.230180</td>\n",
       "      <td>-0.069108</td>\n",
       "      <td>0.047248</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>-0.374561</td>\n",
       "      <td>0.317467</td>\n",
       "      <td>0.332940</td>\n",
       "      <td>-0.634162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127598</th>\n",
       "      <td>business</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.971470</td>\n",
       "      <td>-1.010627</td>\n",
       "      <td>-0.840526</td>\n",
       "      <td>0.260250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594328</td>\n",
       "      <td>-0.323036</td>\n",
       "      <td>-0.563703</td>\n",
       "      <td>-0.511324</td>\n",
       "      <td>-0.014788</td>\n",
       "      <td>-0.490354</td>\n",
       "      <td>0.064403</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>0.332317</td>\n",
       "      <td>0.136097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127599</th>\n",
       "      <td>business</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.055131</td>\n",
       "      <td>0.685632</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>0.652411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160652</td>\n",
       "      <td>-0.149972</td>\n",
       "      <td>-0.062900</td>\n",
       "      <td>-0.177631</td>\n",
       "      <td>-0.405395</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>-0.387202</td>\n",
       "      <td>0.596432</td>\n",
       "      <td>0.542287</td>\n",
       "      <td>-1.034000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127600 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       News Category  Word Count  Noun Phrases  Noun Count  Adjective Count  \\\n",
       "0           business          18             4          12                3   \n",
       "1           business          27             5          15                4   \n",
       "2           business          24             5          17                4   \n",
       "3           business          28             3          19                6   \n",
       "4           business          28             4          16                7   \n",
       "...              ...         ...           ...         ...              ...   \n",
       "127595         world          19             3           9                4   \n",
       "127596        sports          41             7          17                9   \n",
       "127597        sports          20             5           9                3   \n",
       "127598      business          21             4          10                4   \n",
       "127599      business          19             4           8                4   \n",
       "\n",
       "        Verb Count         0         1         2         3  ...        90  \\\n",
       "0                2  0.153226  0.207765  0.341556  0.509732  ...  0.548532   \n",
       "1                3  0.354360 -0.169752  0.193214  0.258507  ... -0.112172   \n",
       "2                2  0.557191 -0.672038 -0.211227  0.216557  ...  0.458479   \n",
       "3                3 -0.862505 -0.846272 -0.091839 -0.208935  ...  0.306875   \n",
       "4                3  0.197830 -0.712709 -0.188731  0.008694  ...  0.394509   \n",
       "...            ...       ...       ...       ...       ...  ...       ...   \n",
       "127595           5 -0.261548 -0.682700 -0.191530 -0.016413  ... -0.206953   \n",
       "127596          10 -0.191443  0.313274 -0.067272  0.118658  ...  0.428284   \n",
       "127597           3 -0.109909  0.097675  0.284736  0.043807  ...  0.510660   \n",
       "127598           4 -0.971470 -1.010627 -0.840526  0.260250  ... -0.594328   \n",
       "127599           4 -0.055131  0.685632  0.449502  0.652411  ...  0.160652   \n",
       "\n",
       "              91        92        93        94        95        96        97  \\\n",
       "0      -0.113774  0.057782 -0.063844  0.389871 -0.095498 -0.000756 -0.519580   \n",
       "1      -0.229674 -0.244061 -0.241459  0.255334 -0.564091 -0.093670 -0.030404   \n",
       "2      -0.680095 -0.661161 -0.101276  0.374314  0.456764  0.762010 -0.061789   \n",
       "3      -1.146071 -0.945447 -0.358710  0.520460  0.908946  0.307853 -0.044196   \n",
       "4      -0.347257 -0.364584 -0.188155  0.747880  0.359722  0.396012  0.213882   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "127595 -0.604427  0.371688 -1.071627 -0.040156 -0.400833 -0.510795 -0.125467   \n",
       "127596 -0.462002 -0.543505  0.149100  0.179041 -0.142714 -0.342634 -0.062155   \n",
       "127597 -0.542429 -0.230180 -0.069108  0.047248  0.005046 -0.374561  0.317467   \n",
       "127598 -0.323036 -0.563703 -0.511324 -0.014788 -0.490354  0.064403  0.324032   \n",
       "127599 -0.149972 -0.062900 -0.177631 -0.405395  0.005585 -0.387202  0.596432   \n",
       "\n",
       "              98        99  \n",
       "0      -0.109446 -0.590428  \n",
       "1      -0.096065 -0.333639  \n",
       "2       0.723553 -0.662003  \n",
       "3       0.347091 -0.568198  \n",
       "4       0.414900 -0.355160  \n",
       "...          ...       ...  \n",
       "127595 -0.329108  0.591055  \n",
       "127596  0.161093 -0.413196  \n",
       "127597  0.332940 -0.634162  \n",
       "127598  0.332317  0.136097  \n",
       "127599  0.542287 -1.034000  \n",
       "\n",
       "[127600 rows x 106 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = pd.DataFrame(word2vec)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "w2v.reset_index(drop=True, inplace=True)\n",
    "#df = pd.concat([df1, df2], axis=1)\n",
    "w2v = pd.concat([data[['News Category','Word Count','Noun Phrases','Noun Count',\n",
    "                                         'Adjective Count','Verb Count']],w2v],axis=1)\n",
    "\n",
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe \n",
    "X = w2v.drop(['News Category'],axis=1) \n",
    "# y series\n",
    "y = w2v['News Category']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "Time to train Random Forest Model: 1.64 mins\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "#forest = forest.fit(trainDataVecs, train_words['News Category'])\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "rf_tt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "print('Time to train Random Forest Model: {} mins'.format(rf_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time()\n",
    "\n",
    "# rf_result = forest.predict(X_test)\n",
    "\n",
    "# rf_pt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "# print('Time taken for label prediction using Random Forest model: {} mins'.format(rf_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = round(accuracy_score(y_test, rf_result)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, rf_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import  confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set(font_scale=1.2)\n",
    "# cof=confusion_matrix(y_test, rf_result)\n",
    "# cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n",
    "# plt.figure(figsize=(7,7))\n",
    "\n",
    "# sns.heatmap(cof, cmap=\"BuPu\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',\n",
    "#             xticklabels=['World','Sports','Business','Science'],\n",
    "#             yticklabels=['World','Sports','Business','Science'])\n",
    "# plt.xlabel(\"\\nPredicted Class\");\n",
    "# plt.ylabel(\"Actual Class\\n\");\n",
    "# # plt.savefig('W2V Confusion Matrix for News Article Classification using Random Forest.png', bbox_inches='tight')\n",
    "# plt.title(\"\\nConfusion Matrix for News Article Classification using Random Forest\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a Naive Bayes to labeled training data...\n",
      "Time to train Naive Bayes Model: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "print(\"Fitting a Naive Bayes to labeled training data...\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "nb_tt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "print('Time to train Naive Bayes Model: {} mins'.format(nb_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time()\n",
    "\n",
    "# nb_result = gnb.predict(X_test)\n",
    "\n",
    "# nb_pt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "# print('Time taken for label prediction using Naive Bayes model: {} mins'.format(nb_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb = round(accuracy_score(y_test, nb_result)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, nb_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# cof=confusion_matrix(y_test, nb_result)\n",
    "# cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n",
    "# plt.figure(figsize=(7,7))\n",
    "\n",
    "# sns.heatmap(cof, cmap=\"BuPu\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',\n",
    "#             xticklabels=['World','Sports','Business','Science'],\n",
    "#             yticklabels=['World','Sports','Business','Science'])\n",
    "# plt.xlabel(\"\\nPredicted Class\");\n",
    "# plt.ylabel(\"Actual Class\\n\");\n",
    "# # plt.savefig('W2V Confusion Matrix for News Article Classification using Naive Bayes.png', bbox_inches='tight')\n",
    "# plt.title(\"\\nConfusion Matrix for News Article Classification using Naive Bayes\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a Logistic Regression model to labeled training data...\n",
      "Time to train Logistic Regression Model: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "\n",
    "print(\"Fitting a Logistic Regression model to labeled training data...\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "logit.fit(X_train,y_train)\n",
    "\n",
    "lr_tt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "print('Time to train Logistic Regression Model: {} mins'.format(lr_tt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time()\n",
    "\n",
    "# lr_result = logit.predict(X_test)\n",
    "\n",
    "# lr_pt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "# print('Time taken for label prediction using Logistic Regression model: {} mins'.format(lr_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = round(accuracy_score(y_test, lr_result)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# cof=confusion_matrix(y_test, lr_result)\n",
    "# cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n",
    "# plt.figure(figsize=(7,7))\n",
    "\n",
    "# sns.heatmap(cof, cmap=\"BuPu\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',\n",
    "#             xticklabels=['World','Sports','Business','Science'],\n",
    "#             yticklabels=['World','Sports','Business','Science'])\n",
    "# plt.xlabel(\"\\nPredicted Class\");\n",
    "# plt.ylabel(\"Actual Class\\n\");\n",
    "# # plt.savefig('W2V Confusion Matrix for News Article Classification using Logistic Regression.png', bbox_inches='tight')\n",
    "# plt.title(\"\\nConfusion Matrix for News Article Classification using Logistic Regression\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a SVM model to labeled training data...\n",
      "Time to train SVM mode: 5.02 mins\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "\n",
    "print(\"Fitting a SVM model to labeled training data...\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "svm_tt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "print('Time to train SVM mode: {} mins'.format(svm_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time()\n",
    "\n",
    "# svm_result = svc.predict(X_test)\n",
    "\n",
    "# svm_pt = round((time() - start_time) / 60, 2)\n",
    "\n",
    "# print('Time taken for label prediction using SVM model: {} mins'.format(svm_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = round(accuracy_score(y_test, svm_result)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# cof=confusion_matrix(y_test, svm_result)\n",
    "# cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n",
    "# plt.figure(figsize=(7,7))\n",
    "\n",
    "# sns.heatmap(cof, cmap=\"BuPu\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',\n",
    "#             xticklabels=['World','Sports','Business','Science'],\n",
    "#             yticklabels=['World','Sports','Business','Science'])\n",
    "# plt.xlabel(\"\\nPredicted Class\");\n",
    "# plt.ylabel(\"Actual Class\\n\");\n",
    "# # plt.savefig('W2V Confusion Matrix for News Article Classification using SVM.png', bbox_inches='tight')\n",
    "# plt.title(\"\\nConfusion Matrix for News Article Classification using SVM\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "# Models = ['RandomForest', 'GaussianNB', 'Logistic', 'SVM']\n",
    "# Accuracy=[rf,nb,lr,svm]\n",
    "# ax.bar(Models,Accuracy,color=[\"lightgreen\",\"pink\", \"skyblue\", \"lightyellow\"]);\n",
    "# for i in ax.patches:\n",
    "#     ax.text(i.get_x()+.1, i.get_height()-5.5, str(round(i.get_height(),2))+'%', fontsize=15, color='black')\n",
    "# plt.title('Comparison of Classification Models trained using word2vec + grammatical + syntax features \\n');\n",
    "# plt.ylabel('Accuracy\\n');\n",
    "# plt.xlabel('\\nClassification Models');\n",
    "# # plt.savefig('W2V Comparison of Classification Models trained using word2vec + grammatical + syntax features.png', bbox_inches='tight')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models (Training Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "# Models = ['RandomForest', 'GaussianNB', 'Logistic', 'SVM']\n",
    "# TrainingTime=[rf_tt,nb_tt,lr_tt,svm_tt]\n",
    "# ax.bar(Models,TrainingTime,color=[\"lightyellow\",\"lightgreen\", \"skyblue\", \"pink\"]);\n",
    "# for i in ax.patches:\n",
    "#     ax.text(i.get_x()+.2, i.get_height(), str(round(i.get_height(),2)), fontsize=14, color='black')\n",
    "# plt.title('Comparison of Different Classification Models Training Time \\n');\n",
    "# plt.ylabel('Training Time (in mins)\\n');\n",
    "# plt.xlabel('\\nClassification Models');\n",
    "# # plt.savefig('W2V Comparison of Different Classification Models Training Time.png', bbox_inches='tight')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models (Label Classification Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.2)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "# Models = ['RandomForest', 'GaussianNB', 'Logistic', 'SVM']\n",
    "# PredictTime=[rf_pt,nb_pt,lr_pt,svm_pt]\n",
    "# ax.bar(Models,PredictTime,color=[\"lightyellow\",\"lightgreen\", \"skyblue\", \"pink\"]);\n",
    "# for i in ax.patches:\n",
    "#     ax.text(i.get_x()+.2, i.get_height(), str(round(i.get_height(),2)), fontsize=14, color='black')\n",
    "# plt.title('Comparison of Different Classification Models Label Classification Time \\n');\n",
    "# plt.ylabel('Label Classification Time (in mins)\\n');\n",
    "# plt.xlabel('\\nClassification Models');\n",
    "# # plt.savefig('W2V Comparison of Different Classification Models Label Classification Time.png', bbox_inches='tight')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation using Cross-Validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# lr_acc = []\n",
    "\n",
    "# for train_index, vaild_index in skf.split(X_train,y_train):\n",
    "#     x_t, x_v = X_train.iloc[train_index], X_train.iloc[vaild_index]\n",
    "#     y_t, y_v = y_train.iloc[train_index], y_train.iloc[vaild_index]\n",
    "#     lr.fit(x_t, y_t)\n",
    "#     lr_acc.append(lr.score(x_v, y_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(lr_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# rf_acc = []\n",
    "\n",
    "# for train_index, vaild_index in skf.split(X_train,y_train):\n",
    "#     x_t, x_v = X_train.iloc[train_index], X_train.iloc[vaild_index]\n",
    "#     y_t, y_v = y_train.iloc[train_index], y_train.iloc[vaild_index]\n",
    "#     rf.fit(x_t, y_t)\n",
    "#     rf_acc.append(rf.score(x_v, y_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(rf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnb = GaussianNB()\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# gnb_acc = []\n",
    "\n",
    "# for train_index, vaild_index in skf.split(X_train,y_train):\n",
    "#     x_t, x_v = X_train.iloc[train_index], X_train.iloc[vaild_index]\n",
    "#     y_t, y_v = y_train.iloc[train_index], y_train.iloc[vaild_index]\n",
    "#     gnb.fit(x_t, y_t)\n",
    "#     gnb_acc.append(gnb.score(x_v, y_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gnb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(gnb_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC()\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "# svc_acc = []\n",
    "\n",
    "# for train_index, vaild_index in skf.split(X_train,y_train):\n",
    "#     x_t, x_v = X_train.iloc[train_index], X_train.iloc[vaild_index]\n",
    "#     y_t, y_v = y_train.iloc[train_index], y_train.iloc[vaild_index]\n",
    "#     svc.fit(x_t, y_t)\n",
    "#     svc_acc.append(svc.score(x_v, y_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(svc_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# y_test = y_test.to_numpy()\n",
    "# y_pred = lr.predict(X_test)\n",
    "\n",
    "# lr_acc = accuracy_score(y_test,y_pred)\n",
    "# lr_recall = recall_score(y_test,y_pred,average='macro')\n",
    "# lr_precision = precision_score(y_test,y_pred,average='macro')\n",
    "# lr_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "# y_pred_roc = OneHotEncoder().fit_transform(y_pred.reshape(-1, 1)).toarray()\n",
    "# y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "# lr_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = gnb.predict(X_test)\n",
    "\n",
    "# gnb_acc = accuracy_score(y_test,y_pred)\n",
    "# gnb_recall = recall_score(y_test_roc,y_pred_roc,average='macro')\n",
    "# gnb_precision = precision_score(y_test_roc,y_pred_roc,average='macro')\n",
    "# gnb_f1 = f1_score(y_test_roc,y_pred_roc,average='macro')\n",
    "\n",
    "# y_pred_roc = OneHotEncoder().fit_transform(y_pred.reshape(-1, 1)).toarray()\n",
    "# y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "# gnb_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')\n",
    "# # The multiclass and multilabel cases expect a shape (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = svc.predict(X_test)\n",
    "\n",
    "# svc_acc = accuracy_score(y_test,y_pred)\n",
    "# svc_recall = recall_score(y_test,y_pred,average='macro')\n",
    "# svc_precision = precision_score(y_test,y_pred,average='macro')\n",
    "# svc_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "# y_pred_roc = OneHotEncoder().fit_transform(y_pred.reshape(-1, 1)).toarray()\n",
    "# y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "# svc_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = rf.predict(X_test)\n",
    "\n",
    "# rf_acc = accuracy_score(y_test,y_pred)\n",
    "# rf_recall = recall_score(y_test,y_pred,average='macro')\n",
    "# rf_precision = precision_score(y_test,y_pred,average='macro')\n",
    "# rf_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "# y_pred_roc = OneHotEncoder().fit_transform(y_pred.reshape(-1, 1)).toarray()\n",
    "# y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "# rf_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Logistic Regression: \",lr_acc,lr_recall,lr_precision,lr_f1,lr_roc)\n",
    "# print(\"Naive Bayes: \", gnb_acc,gnb_recall,gnb_precision,gnb_f1,gnb_roc)\n",
    "# print(\"Support Vector Machine: \", svc_acc,svc_recall,svc_precision,svc_f1,svc_roc)\n",
    "# print(\"Random Forest: \", rf_acc,rf_recall,rf_precision,rf_f1,rf_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame([[lr_acc,lr_recall,lr_precision,lr_f1,lr_roc],\n",
    "#                         [gnb_acc,gnb_recall,gnb_precision,gnb_f1,gnb_roc],\n",
    "#                         [svc_acc,svc_recall,svc_precision,svc_f1,svc_roc],\n",
    "#                         [rf_acc,rf_recall,rf_precision,rf_f1,rf_roc]],\n",
    "#                        columns=['accuracy','recall','precision','fl-score','roc_auc'], \n",
    "#                        index=[\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"Random Forest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv('word2vec_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Testing Data & W2V Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>free agent damontae kazee visit detroit lionyear salary cap mismanagement new atlanta falcon general manager terry fontenot choice purge roster let good player leave via free agency saturday former</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>desean jackson agrees deal ramdesean jackson will always apart philadelphia history he heading home finish career according mike garafalo former eagle secondround pick signing los</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>rocket coach stephen silas heartbreaking press conference 20th straight losshouston rocket case havent noticed now lost stunning twenty straight game falling home oklahoma city thunder one hundred fourteen thousand one hundred twelve sunday team last win came way back</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>rice choreographs murray st past tennessee state three thousand five hundred thirteenpreston rice threw pair touchdown ran another murray state used fourth quarter pull away tennessee state three thousand five hundred thirteen win sunday murray state entered game</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>celtic trade rumor aaron gordon sought multiple team including boston reportceltic got perhaps important extended look one intriguing team ahead thursday’s trade deadline boston downed orlando eleven thousand two hundred ninety six sunday td garden win</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  \\\n",
       "0        sports   \n",
       "1        sports   \n",
       "2        sports   \n",
       "3        sports   \n",
       "4        sports   \n",
       "\n",
       "                                                                                                                                                                                                                                                                      Documents  \\\n",
       "0                                                                         free agent damontae kazee visit detroit lionyear salary cap mismanagement new atlanta falcon general manager terry fontenot choice purge roster let good player leave via free agency saturday former   \n",
       "1                                                                                           desean jackson agrees deal ramdesean jackson will always apart philadelphia history he heading home finish career according mike garafalo former eagle secondround pick signing los   \n",
       "2  rocket coach stephen silas heartbreaking press conference 20th straight losshouston rocket case havent noticed now lost stunning twenty straight game falling home oklahoma city thunder one hundred fourteen thousand one hundred twelve sunday team last win came way back   \n",
       "3       rice choreographs murray st past tennessee state three thousand five hundred thirteenpreston rice threw pair touchdown ran another murray state used fourth quarter pull away tennessee state three thousand five hundred thirteen win sunday murray state entered game   \n",
       "4                  celtic trade rumor aaron gordon sought multiple team including boston reportceltic got perhaps important extended look one intriguing team ahead thursday’s trade deadline boston downed orlando eleven thousand two hundred ninety six sunday td garden win   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          29             4          15                8           5  \n",
       "1          25             6          10                6           5  \n",
       "2          39             6          19                5           8  \n",
       "3          38             7          20                5           7  \n",
       "4          37             5          18                6           6  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('cleaned_bing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[free, agent, damontae, kazee, visit, detroit, lionyear, salary, cap, mismanagement, new, atlanta, falcon, general, manager, terry, fontenot, choice, purge, roster, let, good, player, leave, via, free, agency, saturday, former]</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[desean, jackson, agrees, deal, ramdesean, jackson, will, always, apart, philadelphia, history, he, heading, home, finish, career, according, mike, garafalo, former, eagle, secondround, pick, signing, los]</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[rocket, coach, stephen, silas, heartbreaking, press, conference, 20th, straight, losshouston, rocket, case, havent, noticed, now, lost, stunning, twenty, straight, game, falling, home, oklahoma, city, thunder, one, hundred, fourteen, thousand, one, hundred, twelve, sunday, team, last, win, came, way, back]</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[rice, choreographs, murray, st, past, tennessee, state, three, thousand, five, hundred, thirteenpreston, rice, threw, pair, touchdown, ran, another, murray, state, used, fourth, quarter, pull, away, tennessee, state, three, thousand, five, hundred, thirteen, win, sunday, murray, state, entered, game]</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[celtic, trade, rumor, aaron, gordon, sought, multiple, team, including, boston, reportceltic, got, perhaps, important, extended, look, one, intriguing, team, ahead, thursday’s, trade, deadline, boston, downed, orlando, eleven, thousand, two, hundred, ninety, six, sunday, td, garden, win]</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  \\\n",
       "0        sports   \n",
       "1        sports   \n",
       "2        sports   \n",
       "3        sports   \n",
       "4        sports   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              Documents  \\\n",
       "0                                                                                   [free, agent, damontae, kazee, visit, detroit, lionyear, salary, cap, mismanagement, new, atlanta, falcon, general, manager, terry, fontenot, choice, purge, roster, let, good, player, leave, via, free, agency, saturday, former]   \n",
       "1                                                                                                         [desean, jackson, agrees, deal, ramdesean, jackson, will, always, apart, philadelphia, history, he, heading, home, finish, career, according, mike, garafalo, former, eagle, secondround, pick, signing, los]   \n",
       "2  [rocket, coach, stephen, silas, heartbreaking, press, conference, 20th, straight, losshouston, rocket, case, havent, noticed, now, lost, stunning, twenty, straight, game, falling, home, oklahoma, city, thunder, one, hundred, fourteen, thousand, one, hundred, twelve, sunday, team, last, win, came, way, back]   \n",
       "3        [rice, choreographs, murray, st, past, tennessee, state, three, thousand, five, hundred, thirteenpreston, rice, threw, pair, touchdown, ran, another, murray, state, used, fourth, quarter, pull, away, tennessee, state, three, thousand, five, hundred, thirteen, win, sunday, murray, state, entered, game]   \n",
       "4                     [celtic, trade, rumor, aaron, gordon, sought, multiple, team, including, boston, reportceltic, got, perhaps, important, extended, look, one, intriguing, team, ahead, thursday’s, trade, deadline, boston, downed, orlando, eleven, thousand, two, hundred, ninety, six, sunday, td, garden, win]   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          29             4          15                8           5  \n",
       "1          25             6          10                6           5  \n",
       "2          39             6          19                5           8  \n",
       "3          38             7          20                5           7  \n",
       "4          37             5          18                6           6  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['News Category'] = data['News Category'].replace(\"science\",\"science_and_technology\")\n",
    "data['Documents'] = data['Documents'].str.split(\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:46:06: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-04-29T23:46:06.702436', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# initializing word2vec model\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2, # window size for context \n",
    "                     vector_size=100,  # no of features \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:46:08: collecting all words and their counts\n",
      "INFO - 23:46:08: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 23:46:08: collected 2145 word types from a corpus of 4002 raw words and 144 sentences\n",
      "INFO - 23:46:08: Creating a fresh vocabulary\n",
      "INFO - 23:46:08: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 7 unique words (0.32634032634032634%% of original 2145, drops 2138)', 'datetime': '2021-04-29T23:46:08.348985', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 23:46:08: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 250 word corpus (6.246876561719141%% of original 4002, drops 3752)', 'datetime': '2021-04-29T23:46:08.349300', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 23:46:08: deleting the raw counts dictionary of 2145 items\n",
      "INFO - 23:46:08: sample=6e-05 downsamples 7 most-common words\n",
      "INFO - 23:46:08: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5.18237098552993 word corpus (2.1%% of prior 250)', 'datetime': '2021-04-29T23:46:08.350160', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 23:46:08: estimated required memory for 7 words and 100 dimensions: 9100 bytes\n",
      "INFO - 23:46:08: resetting layer weights\n",
      "INFO - 23:46:08: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-29T23:46:08.351165', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(data['Documents'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:46:08: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 7 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-04-29T23:46:08.881851', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 1 : training on 4002 raw words (5 effective words) took 0.0s, 1819 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 2 : training on 4002 raw words (3 effective words) took 0.0s, 1108 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 3 : training on 4002 raw words (3 effective words) took 0.0s, 1102 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 4 : training on 4002 raw words (8 effective words) took 0.0s, 2641 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 5 : training on 4002 raw words (6 effective words) took 0.0s, 2255 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 6 : training on 4002 raw words (5 effective words) took 0.0s, 2340 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 7 : training on 4002 raw words (5 effective words) took 0.0s, 2331 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 8 : training on 4002 raw words (5 effective words) took 0.0s, 1731 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 9 : training on 4002 raw words (4 effective words) took 0.0s, 1401 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 10 : training on 4002 raw words (6 effective words) took 0.0s, 1694 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 11 : training on 4002 raw words (4 effective words) took 0.0s, 1451 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 12 : training on 4002 raw words (3 effective words) took 0.0s, 940 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 13 : training on 4002 raw words (5 effective words) took 0.0s, 1644 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 14 : training on 4002 raw words (3 effective words) took 0.0s, 569 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 15 : training on 4002 raw words (9 effective words) took 0.0s, 1688 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 16 : training on 4002 raw words (3 effective words) took 0.0s, 754 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 17 : training on 4002 raw words (6 effective words) took 0.0s, 1568 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 18 : training on 4002 raw words (7 effective words) took 0.0s, 2386 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 19 : training on 4002 raw words (5 effective words) took 0.0s, 1538 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 20 : training on 4002 raw words (5 effective words) took 0.0s, 1820 effective words/s\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:08: EPOCH - 21 : training on 4002 raw words (6 effective words) took 0.0s, 1806 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 22 : training on 4002 raw words (5 effective words) took 0.0s, 1436 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 23 : training on 4002 raw words (3 effective words) took 0.0s, 874 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 24 : training on 4002 raw words (8 effective words) took 0.0s, 3328 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 25 : training on 4002 raw words (7 effective words) took 0.0s, 832 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 26 : training on 4002 raw words (5 effective words) took 0.0s, 1841 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 27 : training on 4002 raw words (7 effective words) took 0.0s, 2218 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 28 : training on 4002 raw words (5 effective words) took 0.0s, 1619 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 29 : training on 4002 raw words (6 effective words) took 0.0s, 1876 effective words/s\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:46:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:46:09: EPOCH - 30 : training on 4002 raw words (5 effective words) took 0.0s, 2101 effective words/s\n",
      "INFO - 23:46:09: Word2Vec lifecycle event {'msg': 'training on 120060 raw words (157 effective words) took 0.2s, 957 effective words/s', 'datetime': '2021-04-29T23:46:09.046481', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-142-generic-x86_64-with-glibc2.10', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#train word2vec model \n",
    "model.train(data['Documents'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all headlines \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    feature_vecs = np.zeros((len(words),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for word in words:\n",
    "        feature_vecs[counter] = make_feature_vec(word, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = get_avg_feature_vecs(data['Documents'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of               News Category  Word Count  Noun Phrases  Noun Count  \\\n",
       "1                    sports          25             6          10   \n",
       "2                    sports          39             6          19   \n",
       "3                    sports          38             7          20   \n",
       "4                    sports          37             5          18   \n",
       "6                    sports          33             5          14   \n",
       "..                      ...         ...           ...         ...   \n",
       "132  science_and_technology          28             4          17   \n",
       "133  science_and_technology          27             7          11   \n",
       "137  science_and_technology          23             5          12   \n",
       "140  science_and_technology          30             5          18   \n",
       "143  science_and_technology          35             5          18   \n",
       "\n",
       "     Adjective Count  Verb Count         0         1         2         3  ...  \\\n",
       "1                  6           5 -0.008727  0.002130 -0.000874 -0.009319  ...   \n",
       "2                  5           8 -0.005363  0.004973 -0.002642 -0.000488  ...   \n",
       "3                  5           7 -0.002917  0.002867  0.001351  0.001673  ...   \n",
       "4                  6           6 -0.004340  0.004090  0.000843  0.002889  ...   \n",
       "6                  7           8 -0.004316  0.002604 -0.003843 -0.005347  ...   \n",
       "..               ...         ...       ...       ...       ...       ...  ...   \n",
       "132                4           6 -0.008607  0.003653  0.005210  0.005781  ...   \n",
       "133                7           4 -0.008311  0.009395 -0.000130 -0.001857  ...   \n",
       "137                6           4 -0.008727  0.002130 -0.000874 -0.009319  ...   \n",
       "140                3           2  0.004131 -0.000685 -0.003921 -0.000140  ...   \n",
       "143                5           5  0.000514 -0.001603 -0.004103 -0.000574  ...   \n",
       "\n",
       "           90        91        92        93        94        95        96  \\\n",
       "1    0.009071  0.008938 -0.008208 -0.003012  0.009887  0.005104 -0.001588   \n",
       "2   -0.003337  0.000485  0.004015 -0.004318  0.005895  0.000768  0.006006   \n",
       "3   -0.003120 -0.000752 -0.000766 -0.003754  0.002117  0.001058  0.006768   \n",
       "4   -0.002352  0.000803  0.003492 -0.003646  0.006153  0.003183  0.002178   \n",
       "6    0.002281  0.007320  0.000486 -0.003556  0.008926  0.005240  0.002146   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "132  0.001074 -0.001582  0.002183 -0.007928 -0.002737  0.002684  0.005437   \n",
       "133 -0.007605 -0.001099 -0.000868 -0.002773  0.009768 -0.000390  0.006314   \n",
       "137  0.009071  0.008938 -0.008208 -0.003012  0.009887  0.005104 -0.001588   \n",
       "140 -0.003669  0.003064  0.002783 -0.002331  0.003643  0.003127  0.007215   \n",
       "143  0.000154 -0.002144  0.000834 -0.002398  0.000982 -0.003584  0.007381   \n",
       "\n",
       "           97        98        99  \n",
       "1   -0.008692  0.002962 -0.006676  \n",
       "2   -0.002029  0.000345 -0.002872  \n",
       "3   -0.005410 -0.004739  0.001641  \n",
       "4   -0.004190 -0.000811 -0.000361  \n",
       "6   -0.004090  0.005587 -0.006847  \n",
       "..        ...       ...       ...  \n",
       "132 -0.002447 -0.009575  0.004572  \n",
       "133 -0.007784 -0.002783 -0.005390  \n",
       "137 -0.008692  0.002962 -0.006676  \n",
       "140 -0.002743  0.003177 -0.000640  \n",
       "143 -0.000590 -0.000536  0.004377  \n",
       "\n",
       "[87 rows x 106 columns]>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = pd.DataFrame(word2vec)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "w2v.reset_index(drop=True, inplace=True)\n",
    "w2v = pd.concat([data[['News Category','Word Count','Noun Phrases','Noun Count',\n",
    "                                         'Adjective Count','Verb Count']],w2v],axis=1)\n",
    "\n",
    "w2v.dropna(subset=w2v.columns,inplace=True)\n",
    "w2v.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe \n",
    "X_test = w2v.drop(['News Category'],axis=1) \n",
    "# y series\n",
    "y_test = w2v['News Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                      sports\n",
       "2                      sports\n",
       "3                      sports\n",
       "4                      sports\n",
       "6                      sports\n",
       "                ...          \n",
       "132    science_and_technology\n",
       "133    science_and_technology\n",
       "137    science_and_technology\n",
       "140    science_and_technology\n",
       "143    science_and_technology\n",
       "Name: News Category, Length: 87, dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_test = y_test.to_numpy()\n",
    "y_pred = logit.predict(X_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test,y_pred)\n",
    "lr_recall = recall_score(y_test,y_pred,average='macro')\n",
    "lr_precision = precision_score(y_test,y_pred,average='macro')\n",
    "lr_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "lr_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "gnb_acc = accuracy_score(y_test,y_pred)\n",
    "gnb_recall = recall_score(y_test,y_pred,average='macro')\n",
    "gnb_precision = precision_score(y_test,y_pred,average='macro')\n",
    "gnb_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "gnb_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')\n",
    "# The multiclass and multilabel cases expect a shape (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "svc_acc = accuracy_score(y_test,y_pred)\n",
    "svc_recall = recall_score(y_test,y_pred,average='macro')\n",
    "svc_precision = precision_score(y_test,y_pred,average='macro')\n",
    "svc_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "svc_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test,y_pred)\n",
    "rf_recall = recall_score(y_test,y_pred,average='macro')\n",
    "rf_precision = precision_score(y_test,y_pred,average='macro')\n",
    "rf_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "rf_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.16091954022988506 0.20588235294117646 0.04487179487179487 0.0736842105263158 0.47258403361344536\n",
      "Naive Bayes:  0.25287356321839083 0.20915032679738563 0.09083333333333334 0.12018255578093306 0.47243230625583565\n",
      "Support Vector Machine:  0.20689655172413793 0.2647058823529412 0.11313291139240506 0.12333333333333334 0.5091386554621848\n",
      "Random Forest:  0.19540229885057472 0.25 0.04885057471264368 0.08173076923076923 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: \",lr_acc,lr_recall,lr_precision,lr_f1,lr_roc)\n",
    "print(\"Naive Bayes: \", gnb_acc,gnb_recall,gnb_precision,gnb_f1,gnb_roc)\n",
    "print(\"Support Vector Machine: \", svc_acc,svc_recall,svc_precision,svc_f1,svc_roc)\n",
    "print(\"Random Forest: \", rf_acc,rf_recall,rf_precision,rf_f1,rf_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([[lr_acc,lr_recall,lr_precision,lr_f1,lr_roc],\n",
    "                        [gnb_acc,gnb_recall,gnb_precision,gnb_f1,gnb_roc],\n",
    "                        [svc_acc,svc_recall,svc_precision,svc_f1,svc_roc],\n",
    "                        [rf_acc,rf_recall,rf_precision,rf_f1,rf_roc]],\n",
    "                       columns=['accuracy','recall','precision','fl-score','roc_auc'], \n",
    "                       index=[\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"Random Forest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>fl-score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.160920</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.073684</td>\n",
       "      <td>0.472584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>0.090833</td>\n",
       "      <td>0.120183</td>\n",
       "      <td>0.472432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.113133</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.509139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.081731</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy    recall  precision  fl-score   roc_auc\n",
       "Logistic Regression     0.160920  0.205882   0.044872  0.073684  0.472584\n",
       "Naive Bayes             0.252874  0.209150   0.090833  0.120183  0.472432\n",
       "Support Vector Machine  0.206897  0.264706   0.113133  0.123333  0.509139\n",
       "Random Forest           0.195402  0.250000   0.048851  0.081731  0.500000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('word2vec_bing_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AG_News_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
