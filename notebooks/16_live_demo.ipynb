{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing News API Real Time Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptionKey = os.environ['BING_SEARCH_V7_SUBSCRIPTION_KEY']\n",
    "endpoint = \"https://api.bing.microsoft.com/v7.0/news\"\n",
    "\n",
    "# Construct a request\n",
    "query = \"\"\n",
    "categories = [\"World\", \"Business\", \"Sports\", \"Science\"]\n",
    "count = 100\n",
    "freshness = \"Day\"\n",
    "mkt = 'en-US'\n",
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json'))\n",
    "\n",
    "for category in categories:\n",
    "    params = {'q': query, 'mkt': mkt, 'category': category, 'count': count, 'freshness': freshness}\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscriptionKey}\n",
    "    file_name = datetime.today().strftime('%Y%m%d') + \"_\" + category\n",
    "    \n",
    "    # Call the API\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        #write json string to file\n",
    "        with open(os.path.join(data_file_path,file_name+'.json'), 'w') as json_file:\n",
    "          json.dump(response.json(), json_file)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "    time.sleep(1)\n",
    "    #free account offers only 3 requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import string\n",
    "from num2word import word\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[russell, westbrook, tie, nba, record, unachie...</td>\n",
       "      <td>[westbrook, mindboggling, number, one, major, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[triple, team, bad, rocket, team, quin, snyder...</td>\n",
       "      <td>[know, thought, quin, snyder’s, approach, game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[portland, trail, blazer, continue, winning, b...</td>\n",
       "      <td>[portland, trail, blazer, won, seventh, game, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[outmanned, lightning, fall, panther]</td>\n",
       "      <td>[certainly, wasn’t, way, lightning, wanted, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[instant, analysis, jazz, earn, 50th, win, bea...</td>\n",
       "      <td>[utah, jazz, beat, houston, rocket, one, hundr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                              Title  \\\n",
       "0        sports  [russell, westbrook, tie, nba, record, unachie...   \n",
       "1        sports  [triple, team, bad, rocket, team, quin, snyder...   \n",
       "2        sports  [portland, trail, blazer, continue, winning, b...   \n",
       "3        sports              [outmanned, lightning, fall, panther]   \n",
       "4        sports  [instant, analysis, jazz, earn, 50th, win, bea...   \n",
       "\n",
       "                                         Description  \n",
       "0  [westbrook, mindboggling, number, one, major, ...  \n",
       "1  [know, thought, quin, snyder’s, approach, game...  \n",
       "2  [portland, trail, blazer, won, seventh, game, ...  \n",
       "3  [certainly, wasn’t, way, lightning, wanted, be...  \n",
       "4  [utah, jazz, beat, houston, rocket, one, hundr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json',''))\n",
    "mac = '/'\n",
    "# windows = '\\\\'\n",
    "\n",
    "#dict of json files from bing news api\n",
    "dict = {'sports':[datetime.today().strftime('%Y%m%d')+'_Sports.json'],\n",
    "        'world':[datetime.today().strftime('%Y%m%d')+'_World.json'],\n",
    "        'business':[datetime.today().strftime('%Y%m%d')+'_Business.json'],\n",
    "        'science_and_technology':[datetime.today().strftime('%Y%m%d')+'_Science.json']}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "#iterates through each json file and stores as a dataframe in a list\n",
    "for k, v in dict.items():\n",
    "    for i in v:\n",
    "        init_df = pd.read_json(data_file_path+mac+i)\n",
    "        df = json_normalize(init_df['value'])\n",
    "        df = df[['name','description']]\n",
    "        df.insert(0, 'News Category', k)\n",
    "        df_list.append(df)\n",
    "        df.head()\n",
    "\n",
    "#concatenates list of dataframes into one dataframe\n",
    "data = pd.concat(df_list,axis=0)\n",
    "#renames column title to match other data\n",
    "data = data.rename(columns={'name':'Title', 'description': 'Description'})\n",
    "data.shape\n",
    "data.head()\n",
    "\n",
    "#Below is Fengling's code unedited except adding comments\n",
    "#Remove Punctuation and Stopwords\n",
    "data['Title'] = data['Title'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "data['Description'] = data['Description'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "\n",
    "\n",
    "def convert_num_to_word(words):\n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w.isnumeric():\n",
    "            result.extend(map(lambda x: x.lower(),word(w).split()))\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].str.split().apply(convert_num_to_word)\n",
    "data['Description'] = data['Description'].str.split().apply(convert_num_to_word)\n",
    "\n",
    "\n",
    "def remove_stopword(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_stopword)\n",
    "data['Description'] = data['Description'].apply(remove_stopword)\n",
    "\n",
    "\n",
    "def remove_single_character(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_single_character)\n",
    "data['Description'] = data['Description'].apply(remove_single_character)\n",
    "\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "#this groups words based on their lemma ex: walk v walked v walking\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(lemmatization)\n",
    "data['Description'] = data['Description'].apply(lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "nltk.download()\n",
    "from textblob import TextBlob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>russell westbrook tie nba record unachievable ...</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>triple team bad rocket team quin snyder keep f...</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>portland trail blazer continue winning big one...</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>outmanned lightning fall panther certainly was...</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>instant analysis jazz earn 50th win beating ro...</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                          Documents  \\\n",
       "0        sports  russell westbrook tie nba record unachievable ...   \n",
       "1        sports  triple team bad rocket team quin snyder keep f...   \n",
       "2        sports  portland trail blazer continue winning big one...   \n",
       "3        sports  outmanned lightning fall panther certainly was...   \n",
       "4        sports  instant analysis jazz earn 50th win beating ro...   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          27             5          15                7           2  \n",
       "1          28             6          16                4           7  \n",
       "2          48             5          15               10           9  \n",
       "3          23             4          10                5           7  \n",
       "4          37             6          18                6           6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents'] = data['Title'] + data['Description']\n",
    "data.drop(['Title','Description'],axis=1,inplace=True)\n",
    "data['Documents'] = data['Documents'].apply(lambda x: ' '.join(x))\n",
    "data['Documents'] = data['Documents'].apply(lambda x: x.replace(\"\\'\",\"\").replace(',','').replace(']','').replace('[',''))\n",
    "data['Word Count'] = data['Documents'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "data['Noun Phrases'] = data['Documents'].apply(lambda x: len(TextBlob(x).noun_phrases))\n",
    "data['Tags'] = data['Documents'].apply(lambda t: collections.Counter(tag for word,tag in TextBlob(t).tags))\n",
    "\n",
    "# pos tag list https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "\n",
    "data['Noun Count'] = data['Tags'].apply(lambda d: d.get('NN',0)+d.get('NNS',0)+d.get('NNP',0)+d.get('NNPS',0))\n",
    "data['Adjective Count'] = data['Tags'].apply(lambda d: d.get('JJ',0)+d.get('JJR',0)+d.get('JJS',0))\n",
    "data['Verb Count'] = data['Tags'].apply(lambda d: d.get('VB',0)+d.get('VBD',0)+d.get('VBG',0)+d.get('VBN',0)+d.get('VBP',0)+d.get('VBZ',0))\n",
    "data.drop(['Tags'],axis=1,inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time \n",
    "import multiprocessing\n",
    "import logging  # logger\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:22:18: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-05-09T01:22:18.134286', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# initializing word2vec model\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2, # window size for context \n",
    "                     vector_size=100,  # no of features \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:22:18: collecting all words and their counts\n",
      "WARNING - 01:22:18: Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "INFO - 01:22:18: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 01:22:18: collected 39 word types from a corpus of 9135 raw words and 48 sentences\n",
      "INFO - 01:22:18: Creating a fresh vocabulary\n",
      "INFO - 01:22:18: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 25 unique words (64.1025641025641%% of original 39, drops 14)', 'datetime': '2021-05-09T01:22:18.166132', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 01:22:18: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 9042 word corpus (98.98193760262726%% of original 9135, drops 93)', 'datetime': '2021-05-09T01:22:18.166785', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 01:22:18: deleting the raw counts dictionary of 39 items\n",
      "INFO - 01:22:18: sample=6e-05 downsamples 25 most-common words\n",
      "INFO - 01:22:18: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 335.33184932217915 word corpus (3.7%% of prior 9042)', 'datetime': '2021-05-09T01:22:18.168942', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 01:22:18: estimated required memory for 25 words and 100 dimensions: 32500 bytes\n",
      "INFO - 01:22:18: resetting layer weights\n",
      "INFO - 01:22:18: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-05-09T01:22:18.171910', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(data['Documents'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:22:18: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 25 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-05-09T01:22:18.178959', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 1 : training on 9135 raw words (305 effective words) took 0.0s, 50803 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 2 : training on 9135 raw words (340 effective words) took 0.0s, 62440 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 3 : training on 9135 raw words (323 effective words) took 0.0s, 44876 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 4 : training on 9135 raw words (342 effective words) took 0.0s, 52022 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 5 : training on 9135 raw words (327 effective words) took 0.0s, 34846 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 6 : training on 9135 raw words (336 effective words) took 0.0s, 47492 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 7 : training on 9135 raw words (347 effective words) took 0.0s, 49184 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 8 : training on 9135 raw words (302 effective words) took 0.0s, 51436 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 9 : training on 9135 raw words (347 effective words) took 0.0s, 45580 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 10 : training on 9135 raw words (310 effective words) took 0.0s, 37272 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 11 : training on 9135 raw words (317 effective words) took 0.0s, 68491 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 12 : training on 9135 raw words (337 effective words) took 0.0s, 43966 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 13 : training on 9135 raw words (331 effective words) took 0.0s, 51553 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 14 : training on 9135 raw words (313 effective words) took 0.0s, 58009 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 15 : training on 9135 raw words (330 effective words) took 0.0s, 53961 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 16 : training on 9135 raw words (351 effective words) took 0.0s, 59070 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 17 : training on 9135 raw words (336 effective words) took 0.0s, 41126 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 18 : training on 9135 raw words (345 effective words) took 0.0s, 52716 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 19 : training on 9135 raw words (304 effective words) took 0.0s, 57347 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 20 : training on 9135 raw words (320 effective words) took 0.0s, 54054 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 21 : training on 9135 raw words (331 effective words) took 0.0s, 51028 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 22 : training on 9135 raw words (336 effective words) took 0.0s, 68279 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 23 : training on 9135 raw words (330 effective words) took 0.0s, 65426 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 24 : training on 9135 raw words (330 effective words) took 0.0s, 57249 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 25 : training on 9135 raw words (309 effective words) took 0.0s, 56259 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 26 : training on 9135 raw words (323 effective words) took 0.0s, 67690 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 27 : training on 9135 raw words (327 effective words) took 0.0s, 49269 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 28 : training on 9135 raw words (330 effective words) took 0.0s, 56975 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 29 : training on 9135 raw words (299 effective words) took 0.0s, 56520 effective words/s\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 01:22:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 01:22:18: EPOCH - 30 : training on 9135 raw words (311 effective words) took 0.0s, 47193 effective words/s\n",
      "INFO - 01:22:18: Word2Vec lifecycle event {'msg': 'training on 274050 raw words (9789 effective words) took 0.4s, 25221 effective words/s', 'datetime': '2021-05-09T01:22:18.568275', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#train word2vec model \n",
    "model.train(data['Documents'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all headlines \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    feature_vecs = np.zeros((len(words),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for word in words:\n",
    "        feature_vecs[counter] = make_feature_vec(word, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = get_avg_feature_vecs(data['Documents'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(word2vec))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    word2vec = np.delete(word2vec, nan_indices, axis=0)\n",
    "    word2vec.drop(data.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert word2vec.shape[0] == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.139191</td>\n",
       "      <td>0.172972</td>\n",
       "      <td>0.097528</td>\n",
       "      <td>0.109247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275901</td>\n",
       "      <td>0.138667</td>\n",
       "      <td>0.134518</td>\n",
       "      <td>-0.016166</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>0.078404</td>\n",
       "      <td>0.222203</td>\n",
       "      <td>-0.270796</td>\n",
       "      <td>0.078494</td>\n",
       "      <td>0.137154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.140603</td>\n",
       "      <td>0.174125</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>0.108240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276365</td>\n",
       "      <td>0.139580</td>\n",
       "      <td>0.134864</td>\n",
       "      <td>-0.017174</td>\n",
       "      <td>0.379521</td>\n",
       "      <td>0.079596</td>\n",
       "      <td>0.221663</td>\n",
       "      <td>-0.271183</td>\n",
       "      <td>0.078286</td>\n",
       "      <td>0.136707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.139865</td>\n",
       "      <td>0.173593</td>\n",
       "      <td>0.097847</td>\n",
       "      <td>0.107312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276472</td>\n",
       "      <td>0.139418</td>\n",
       "      <td>0.135319</td>\n",
       "      <td>-0.017005</td>\n",
       "      <td>0.378967</td>\n",
       "      <td>0.079099</td>\n",
       "      <td>0.222359</td>\n",
       "      <td>-0.271193</td>\n",
       "      <td>0.078933</td>\n",
       "      <td>0.136285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.140761</td>\n",
       "      <td>0.174590</td>\n",
       "      <td>0.096981</td>\n",
       "      <td>0.106697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>0.140179</td>\n",
       "      <td>0.135892</td>\n",
       "      <td>-0.016939</td>\n",
       "      <td>0.379782</td>\n",
       "      <td>0.078771</td>\n",
       "      <td>0.222114</td>\n",
       "      <td>-0.270521</td>\n",
       "      <td>0.078998</td>\n",
       "      <td>0.136414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.140718</td>\n",
       "      <td>0.174285</td>\n",
       "      <td>0.097562</td>\n",
       "      <td>0.107728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276479</td>\n",
       "      <td>0.139207</td>\n",
       "      <td>0.134532</td>\n",
       "      <td>-0.017244</td>\n",
       "      <td>0.379008</td>\n",
       "      <td>0.078861</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>-0.271424</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.136367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  Word Count  Noun Phrases  Noun Count  Adjective Count  \\\n",
       "0        sports          27             5          15                7   \n",
       "1        sports          28             6          16                4   \n",
       "2        sports          48             5          15               10   \n",
       "3        sports          23             4          10                5   \n",
       "4        sports          37             6          18                6   \n",
       "\n",
       "   Verb Count         0         1         2         3  ...        90  \\\n",
       "0           2 -0.139191  0.172972  0.097528  0.109247  ...  0.275901   \n",
       "1           7 -0.140603  0.174125  0.097980  0.108240  ...  0.276365   \n",
       "2           9 -0.139865  0.173593  0.097847  0.107312  ...  0.276472   \n",
       "3           7 -0.140761  0.174590  0.096981  0.106697  ...  0.276483   \n",
       "4           6 -0.140718  0.174285  0.097562  0.107728  ...  0.276479   \n",
       "\n",
       "         91        92        93        94        95        96        97  \\\n",
       "0  0.138667  0.134518 -0.016166  0.378698  0.078404  0.222203 -0.270796   \n",
       "1  0.139580  0.134864 -0.017174  0.379521  0.079596  0.221663 -0.271183   \n",
       "2  0.139418  0.135319 -0.017005  0.378967  0.079099  0.222359 -0.271193   \n",
       "3  0.140179  0.135892 -0.016939  0.379782  0.078771  0.222114 -0.270521   \n",
       "4  0.139207  0.134532 -0.017244  0.379008  0.078861  0.221922 -0.271424   \n",
       "\n",
       "         98        99  \n",
       "0  0.078494  0.137154  \n",
       "1  0.078286  0.136707  \n",
       "2  0.078933  0.136285  \n",
       "3  0.078998  0.136414  \n",
       "4  0.078358  0.136367  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = pd.DataFrame(word2vec)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "w2v.reset_index(drop=True, inplace=True)\n",
    "#df = pd.concat([df1, df2], axis=1)\n",
    "w2v = pd.concat([data[['News Category','Word Count','Noun Phrases','Noun Count',\n",
    "                                         'Adjective Count','Verb Count']],w2v],axis=1)\n",
    "\n",
    "w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkl_Filename = \"Pickle_RL_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LR Model on Real-Time Data with W2V Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe \n",
    "X_test = w2v.drop(['News Category'],axis=1) \n",
    "# y series\n",
    "y_test = w2v['News Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.22916666666666666 0.22916666666666666 0.059782608695652176 0.09482758620689655 0.48611111111111116\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.to_numpy()\n",
    "y_pred = Pickled_LR_Model.predict(X_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test,y_pred)\n",
    "lr_recall = recall_score(y_test,y_pred,average='macro')\n",
    "lr_precision = precision_score(y_test,y_pred,average='macro')\n",
    "lr_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "lr_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')\n",
    "\n",
    "print(\"Logistic Regression: \",lr_acc,lr_recall,lr_precision,lr_f1,lr_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
