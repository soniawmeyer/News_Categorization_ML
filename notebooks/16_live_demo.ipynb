{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing News API Real Time Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls 10 articles from each of our 4 news categories from the bing news api\n",
    "subscriptionKey = os.environ['BING_SEARCH_V7_SUBSCRIPTION_KEY']\n",
    "endpoint = \"https://api.bing.microsoft.com/v7.0/news\"\n",
    "\n",
    "# Construct a request\n",
    "query = \"\"\n",
    "categories = [\"World\", \"Business\", \"Sports\", \"Science\"]\n",
    "count = 100\n",
    "freshness = \"Day\"\n",
    "mkt = 'en-US'\n",
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json'))\n",
    "\n",
    "for category in categories:\n",
    "    params = {'q': query, 'mkt': mkt, 'category': category, 'count': count, 'freshness': freshness}\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscriptionKey}\n",
    "    file_name = datetime.today().strftime('%Y%m%d') + \"_\" + category\n",
    "    \n",
    "    # Call the API\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        #write json string to file\n",
    "        with open(os.path.join(data_file_path,file_name+'.json'), 'w') as json_file:\n",
    "          json.dump(response.json(), json_file)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "\n",
    "    time.sleep(1)\n",
    "    #free account offers only 3 requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import string\n",
    "from num2word import word\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[dana, white, doesnt, care, jon, jones, fight,...</td>\n",
       "      <td>[ufc, president, dana, white, continues, stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[tennessee, titan, offseason, roster, among, o...</td>\n",
       "      <td>[tennessee, titan, offseason, roster, will, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[giant, open, offense, rookie, receiver, kadar...</td>\n",
       "      <td>[east, rutherford, new, jersey, usa, new, york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[grizzly, v, warrior, betting, pick, predictio...</td>\n",
       "      <td>[sunday, golden, state, warrior, memphis, griz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[chris, webber, elected, naismith, memorial, h...</td>\n",
       "      <td>[saturday, first, wolverine, enshrined, spring...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                              Title  \\\n",
       "0        sports  [dana, white, doesnt, care, jon, jones, fight,...   \n",
       "1        sports  [tennessee, titan, offseason, roster, among, o...   \n",
       "2        sports  [giant, open, offense, rookie, receiver, kadar...   \n",
       "3        sports  [grizzly, v, warrior, betting, pick, predictio...   \n",
       "4        sports  [chris, webber, elected, naismith, memorial, h...   \n",
       "\n",
       "                                         Description  \n",
       "0  [ufc, president, dana, white, continues, stand...  \n",
       "1  [tennessee, titan, offseason, roster, will, se...  \n",
       "2  [east, rutherford, new, jersey, usa, new, york...  \n",
       "3  [sunday, golden, state, warrior, memphis, griz...  \n",
       "4  [saturday, first, wolverine, enshrined, spring...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path = os.path.abspath(os.path.join(os.pardir,'data','bing_api_json',''))\n",
    "mac = '/'\n",
    "# windows = '\\\\'\n",
    "\n",
    "#dict of json files from bing news api\n",
    "dict = {'sports':[datetime.today().strftime('%Y%m%d')+'_Sports.json'],\n",
    "        'world':[datetime.today().strftime('%Y%m%d')+'_World.json'],\n",
    "        'business':[datetime.today().strftime('%Y%m%d')+'_Business.json'],\n",
    "        'science_and_technology':[datetime.today().strftime('%Y%m%d')+'_Science.json']}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "#iterates through each json file and stores as a dataframe in a list\n",
    "for k, v in dict.items():\n",
    "    for i in v:\n",
    "        init_df = pd.read_json(data_file_path+mac+i)\n",
    "        df = json_normalize(init_df['value'])\n",
    "        df = df[['name','description']]\n",
    "        df.insert(0, 'News Category', k)\n",
    "        df_list.append(df)\n",
    "        df.head()\n",
    "\n",
    "#concatenates list of dataframes into one dataframe\n",
    "data = pd.concat(df_list,axis=0)\n",
    "#renames column title to match other data\n",
    "data = data.rename(columns={'name':'Title', 'description': 'Description'})\n",
    "data.shape\n",
    "data.head()\n",
    "\n",
    "#Below is Fengling's code unedited except adding comments\n",
    "#Remove Punctuation and Stopwords\n",
    "data['Title'] = data['Title'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "data['Description'] = data['Description'].str.translate(str.maketrans('','',string.punctuation)).str.lower()\n",
    "\n",
    "\n",
    "def convert_num_to_word(words):\n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w.isnumeric():\n",
    "            result.extend(map(lambda x: x.lower(),word(w).split()))\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].str.split().apply(convert_num_to_word)\n",
    "data['Description'] = data['Description'].str.split().apply(convert_num_to_word)\n",
    "\n",
    "\n",
    "def remove_stopword(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORDS:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_stopword)\n",
    "data['Description'] = data['Description'].apply(remove_stopword)\n",
    "\n",
    "\n",
    "def remove_single_character(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(remove_single_character)\n",
    "data['Description'] = data['Description'].apply(remove_single_character)\n",
    "\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "#this groups words based on their lemma ex: walk v walked v walking\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "data['Title'] = data['Title'].apply(lemmatization)\n",
    "data['Description'] = data['Description'].apply(lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "nltk.download()\n",
    "from textblob import TextBlob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>dana white doesnt care jon jones fight ufc tha...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>tennessee titan offseason roster among oldest ...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>giant open offense rookie receiver kadarius to...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>grizzly v warrior betting pick prediction poin...</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>chris webber elected naismith memorial hall fa...</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category                                          Documents  \\\n",
       "0        sports  dana white doesnt care jon jones fight ufc tha...   \n",
       "1        sports  tennessee titan offseason roster among oldest ...   \n",
       "2        sports  giant open offense rookie receiver kadarius to...   \n",
       "3        sports  grizzly v warrior betting pick prediction poin...   \n",
       "4        sports  chris webber elected naismith memorial hall fa...   \n",
       "\n",
       "   Word Count  Noun Phrases  Noun Count  Adjective Count  Verb Count  \n",
       "0          30             4          13               11           5  \n",
       "1          30             4          14                5           5  \n",
       "2          28             5          18                7           1  \n",
       "3          36             7          18                8           7  \n",
       "4          25             4          12                8           3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Documents'] = data['Title'] + data['Description']\n",
    "data.drop(['Title','Description'],axis=1,inplace=True)\n",
    "data['Documents'] = data['Documents'].apply(lambda x: ' '.join(x))\n",
    "data['Documents'] = data['Documents'].apply(lambda x: x.replace(\"\\'\",\"\").replace(',','').replace(']','').replace('[',''))\n",
    "data['Word Count'] = data['Documents'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "data['Noun Phrases'] = data['Documents'].apply(lambda x: len(TextBlob(x).noun_phrases))\n",
    "data['Tags'] = data['Documents'].apply(lambda t: collections.Counter(tag for word,tag in TextBlob(t).tags))\n",
    "\n",
    "# pos tag list https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "\n",
    "data['Noun Count'] = data['Tags'].apply(lambda d: d.get('NN',0)+d.get('NNS',0)+d.get('NNP',0)+d.get('NNPS',0))\n",
    "data['Adjective Count'] = data['Tags'].apply(lambda d: d.get('JJ',0)+d.get('JJR',0)+d.get('JJS',0))\n",
    "data['Verb Count'] = data['Tags'].apply(lambda d: d.get('VB',0)+d.get('VBD',0)+d.get('VBG',0)+d.get('VBN',0)+d.get('VBP',0)+d.get('VBZ',0))\n",
    "data.drop(['Tags'],axis=1,inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time \n",
    "import multiprocessing\n",
    "import logging  # logger\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:42:20: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-05-16T11:42:20.799873', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# initializing word2vec model\n",
    "#shallow neural network model\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2, # window size for context \n",
    "                     vector_size=100,  # no of features \n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:42:20: collecting all words and their counts\n",
      "WARNING - 11:42:20: Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "INFO - 11:42:20: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:42:20: collected 36 word types from a corpus of 9281 raw words and 48 sentences\n",
      "INFO - 11:42:20: Creating a fresh vocabulary\n",
      "INFO - 11:42:20: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 25 unique words (69.44444444444444%% of original 36, drops 11)', 'datetime': '2021-05-16T11:42:20.838528', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 11:42:20: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 9213 word corpus (99.26732033186079%% of original 9281, drops 68)', 'datetime': '2021-05-16T11:42:20.839077', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 11:42:20: deleting the raw counts dictionary of 36 items\n",
      "INFO - 11:42:20: sample=6e-05 downsamples 25 most-common words\n",
      "INFO - 11:42:20: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 340.54451464691317 word corpus (3.7%% of prior 9213)', 'datetime': '2021-05-16T11:42:20.840918', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 11:42:20: estimated required memory for 25 words and 100 dimensions: 32500 bytes\n",
      "INFO - 11:42:20: resetting layer weights\n",
      "INFO - 11:42:20: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-05-16T11:42:20.844273', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# build vocabulary  and learns word associations in text\n",
    "model.build_vocab(data['Documents'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:42:20: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 25 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-05-16T11:42:20.851930', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 1 : training on 9281 raw words (313 effective words) took 0.0s, 63621 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 2 : training on 9281 raw words (392 effective words) took 0.0s, 58478 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 3 : training on 9281 raw words (329 effective words) took 0.0s, 81170 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 4 : training on 9281 raw words (296 effective words) took 0.0s, 40717 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 5 : training on 9281 raw words (345 effective words) took 0.0s, 62786 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 6 : training on 9281 raw words (334 effective words) took 0.0s, 44416 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 7 : training on 9281 raw words (326 effective words) took 0.0s, 51934 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 8 : training on 9281 raw words (320 effective words) took 0.0s, 49181 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 9 : training on 9281 raw words (339 effective words) took 0.0s, 66817 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 10 : training on 9281 raw words (329 effective words) took 0.0s, 70401 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 11 : training on 9281 raw words (319 effective words) took 0.0s, 63128 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 12 : training on 9281 raw words (310 effective words) took 0.0s, 47292 effective words/s\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:42:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:20: EPOCH - 13 : training on 9281 raw words (343 effective words) took 0.0s, 64242 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 14 : training on 9281 raw words (335 effective words) took 0.0s, 62317 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 15 : training on 9281 raw words (352 effective words) took 0.0s, 60089 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 16 : training on 9281 raw words (354 effective words) took 0.0s, 51310 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 17 : training on 9281 raw words (360 effective words) took 0.0s, 33759 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 18 : training on 9281 raw words (343 effective words) took 0.0s, 40100 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 19 : training on 9281 raw words (318 effective words) took 0.0s, 50322 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 20 : training on 9281 raw words (349 effective words) took 0.0s, 54231 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 21 : training on 9281 raw words (340 effective words) took 0.0s, 60037 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 22 : training on 9281 raw words (349 effective words) took 0.0s, 58254 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 23 : training on 9281 raw words (335 effective words) took 0.0s, 46814 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 24 : training on 9281 raw words (386 effective words) took 0.0s, 64209 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 25 : training on 9281 raw words (316 effective words) took 0.0s, 49681 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 26 : training on 9281 raw words (337 effective words) took 0.0s, 59269 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 27 : training on 9281 raw words (330 effective words) took 0.0s, 58741 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 28 : training on 9281 raw words (330 effective words) took 0.0s, 66719 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 29 : training on 9281 raw words (360 effective words) took 0.0s, 69803 effective words/s\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:42:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:42:21: EPOCH - 30 : training on 9281 raw words (345 effective words) took 0.0s, 64141 effective words/s\n",
      "INFO - 11:42:21: Word2Vec lifecycle event {'msg': 'training on 278430 raw words (10134 effective words) took 0.4s, 26800 effective words/s', 'datetime': '2021-05-16T11:42:21.231041', 'gensim': '4.0.1', 'python': '3.7.5 (default, Oct 25 2019, 10:52:18) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-19.6.0-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#train word2vec model \n",
    "model.train(data['Documents'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all headlines \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    feature_vecs = np.zeros((len(words),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for word in words:\n",
    "        feature_vecs[counter] = make_feature_vec(word, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = get_avg_feature_vecs(data['Documents'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(word2vec))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    word2vec = np.delete(word2vec, nan_indices, axis=0)\n",
    "    word2vec.drop(data.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert word2vec.shape[0] == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Category</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Noun Count</th>\n",
       "      <th>Adjective Count</th>\n",
       "      <th>Verb Count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.077073</td>\n",
       "      <td>0.249286</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>0.232694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229867</td>\n",
       "      <td>0.186848</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.508744</td>\n",
       "      <td>0.057262</td>\n",
       "      <td>-0.016012</td>\n",
       "      <td>-0.255578</td>\n",
       "      <td>0.072951</td>\n",
       "      <td>0.111253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.077828</td>\n",
       "      <td>0.249213</td>\n",
       "      <td>0.141380</td>\n",
       "      <td>0.235164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229121</td>\n",
       "      <td>0.186928</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.142918</td>\n",
       "      <td>0.509141</td>\n",
       "      <td>0.057069</td>\n",
       "      <td>-0.017085</td>\n",
       "      <td>-0.255837</td>\n",
       "      <td>0.073274</td>\n",
       "      <td>0.111010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.077331</td>\n",
       "      <td>0.249805</td>\n",
       "      <td>0.141737</td>\n",
       "      <td>0.234225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.186665</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.141685</td>\n",
       "      <td>0.509070</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>-0.015737</td>\n",
       "      <td>-0.255995</td>\n",
       "      <td>0.073331</td>\n",
       "      <td>0.110955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.077276</td>\n",
       "      <td>0.249513</td>\n",
       "      <td>0.141506</td>\n",
       "      <td>0.234059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229165</td>\n",
       "      <td>0.187440</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.142213</td>\n",
       "      <td>0.509231</td>\n",
       "      <td>0.057501</td>\n",
       "      <td>-0.016305</td>\n",
       "      <td>-0.255746</td>\n",
       "      <td>0.073567</td>\n",
       "      <td>0.110787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.077441</td>\n",
       "      <td>0.250204</td>\n",
       "      <td>0.140386</td>\n",
       "      <td>0.233805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228812</td>\n",
       "      <td>0.187703</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.142848</td>\n",
       "      <td>0.509732</td>\n",
       "      <td>0.056591</td>\n",
       "      <td>-0.015792</td>\n",
       "      <td>-0.255740</td>\n",
       "      <td>0.072936</td>\n",
       "      <td>0.111116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Category  Word Count  Noun Phrases  Noun Count  Adjective Count  \\\n",
       "0        sports          30             4          13               11   \n",
       "1        sports          30             4          14                5   \n",
       "2        sports          28             5          18                7   \n",
       "3        sports          36             7          18                8   \n",
       "4        sports          25             4          12                8   \n",
       "\n",
       "   Verb Count         0         1         2         3  ...        90  \\\n",
       "0           5 -0.077073  0.249286  0.141428  0.232694  ...  0.229867   \n",
       "1           5 -0.077828  0.249213  0.141380  0.235164  ...  0.229121   \n",
       "2           1 -0.077331  0.249805  0.141737  0.234225  ...  0.229294   \n",
       "3           7 -0.077276  0.249513  0.141506  0.234059  ...  0.229165   \n",
       "4           3 -0.077441  0.250204  0.140386  0.233805  ...  0.228812   \n",
       "\n",
       "         91        92        93        94        95        96        97  \\\n",
       "0  0.186848  0.002071  0.142191  0.508744  0.057262 -0.016012 -0.255578   \n",
       "1  0.186928  0.001202  0.142918  0.509141  0.057069 -0.017085 -0.255837   \n",
       "2  0.186665  0.001828  0.141685  0.509070  0.057436 -0.015737 -0.255995   \n",
       "3  0.187440  0.001823  0.142213  0.509231  0.057501 -0.016305 -0.255746   \n",
       "4  0.187703  0.002199  0.142848  0.509732  0.056591 -0.015792 -0.255740   \n",
       "\n",
       "         98        99  \n",
       "0  0.072951  0.111253  \n",
       "1  0.073274  0.111010  \n",
       "2  0.073331  0.110955  \n",
       "3  0.073567  0.110787  \n",
       "4  0.072936  0.111116  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = pd.DataFrame(word2vec)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "w2v.reset_index(drop=True, inplace=True)\n",
    "#df = pd.concat([df1, df2], axis=1)\n",
    "w2v = pd.concat([data[['News Category','Word Count','Noun Phrases','Noun Count',\n",
    "                                         'Adjective Count','Verb Count']],w2v],axis=1)\n",
    "\n",
    "w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkl_Filename = \"Pickle_RL_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LR Model on Real-Time Data with W2V Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe \n",
    "X_test = w2v.drop(['News Category'],axis=1) \n",
    "# y series\n",
    "y_test = w2v['News Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.25 0.25 0.0625 0.1 0.5\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.to_numpy()\n",
    "y_pred = Pickled_LR_Model.predict(X_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test,y_pred)\n",
    "lr_recall = recall_score(y_test,y_pred,average='macro')\n",
    "lr_precision = precision_score(y_test,y_pred,average='macro')\n",
    "lr_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "\n",
    "y_pred_roc = OneHotEncoder().fit(y_test.reshape(-1, 1)).transform(y_pred.reshape(-1,1)).toarray()\n",
    "y_test_roc = OneHotEncoder().fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "lr_roc = roc_auc_score(y_test_roc,y_pred_roc,multi_class='ovo')\n",
    "\n",
    "print(\"Logistic Regression: \",lr_acc,lr_recall,lr_precision,lr_f1,lr_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
